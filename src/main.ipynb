{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qji7gxIRzr_D"
      },
      "source": [
        "# **Full GPU-Accelerated Variational AutoEncoder Implementation in CUDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQBCQoJHzIN9"
      },
      "source": [
        "## Experimental Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9t8wuMq2woN"
      },
      "source": [
        "In this section we prepare and validate the experimental environment used\n",
        "for all subsequent benchmarks and analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63YL0obGzNQ8"
      },
      "source": [
        "We first verify that a CUDA-capable GPU is available and that the CUDA compiler (`nvcc`) is correctly installed.  \n",
        "This step ensures that the benchmarks will run on the expected hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKz_kz6MBa0J"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpLN-GHmFiMi"
      },
      "outputs": [],
      "source": [
        "!apt-get update -y\n",
        "!apt-get install -y nsight-systems-2025.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unTXbIgYzU1p"
      },
      "source": [
        "We clone the project repository from GitHub and place it in the working directory.\n",
        "This step recreates the exact codebase used for the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K45c9LQ7BzLG"
      },
      "outputs": [],
      "source": [
        "REPO_URL=\"https://github.com/massimo-ruggiero/vae-cuda\"\n",
        "PROJECT_DIR=\"VAE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGD7WPyRCY8E"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf \"$PROJECT_DIR\"\n",
        "!git clone --depth 1 \"$REPO_URL\" \"$PROJECT_DIR\"\n",
        "%cd \"$PROJECT_DIR\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Ga-Oy1zcO_"
      },
      "source": [
        "We inspect the directory structure of the repository to verify that all expected\n",
        "modules and scripts are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwshBjK9CcjI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update -y >/dev/null\n",
        "!sudo apt-get install -y tree >/dev/null\n",
        "!tree -L 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EziMFeTZzh2z"
      },
      "source": [
        "The repository includes helper scripts for running the main training pipeline, the *micro* and *macro* benchmark suite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stkYEKr5CfdL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!ls -la scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7405GwBPzqhK"
      },
      "source": [
        "The VAE implementation expects the MNIST dataset to be provided in a custom\n",
        "binary format for fast loading during training and benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJh1WC1jMxPx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def save_to_bin(images, labels, filename):\n",
        "    images_flat = images.reshape(images.shape[0], -1).astype(np.uint8)\n",
        "    labels = labels.astype(np.uint8)\n",
        "\n",
        "    num_samples = images.shape[0]\n",
        "\n",
        "    header = np.array([num_samples], dtype=np.int32)\n",
        "\n",
        "    print(f\"Scrittura {filename}...\")\n",
        "    print(f\"  - Samples: {num_samples}\")\n",
        "    print(f\"  - Dimensioni Dati: {images_flat.shape}\")\n",
        "    print(f\"  - Dimensioni Labels: {labels.shape}\")\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        header.tofile(f)\n",
        "        images_flat.tofile(f)\n",
        "        labels.tofile(f)\n",
        "\n",
        "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
        "    print(f\"  -> Completato! ({size_mb:.2f} MB)\\n\")\n",
        "\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "    print(\"Cartella 'data/' creata.\")\n",
        "\n",
        "print(\"Scaricamento MNIST da Keras...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Genera i file binari\n",
        "save_to_bin(x_train, y_train, 'data/train.bin')\n",
        "save_to_bin(x_test, y_test,  'data/test.bin')\n",
        "\n",
        "print(\"Tutto fatto. Ora puoi lanciare il programma C++.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aue81QwyedQ"
      },
      "source": [
        "## End-to-End Sanity Check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9mdRUQ821RM"
      },
      "source": [
        "Before running the full benchmark suite, we perform a quick end-to-end test to verify that:\n",
        "- the project compiles and runs correctly on the current GPU\n",
        "- training executes without runtime errors\n",
        "- the VAE produces a valid reconstruction\n",
        "- the sampling pipeline generates plausible outputs\n",
        "\n",
        "This step is not meant to optimize performance: it is a correctness + pipeline validation check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3ZuH5JVHF-3"
      },
      "outputs": [],
      "source": [
        "!chmod +x scripts/run_sanity_check.sh\n",
        "!bash scripts/run_sanity_check.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ednULfeUNLi4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "IMG_SIZE = 28\n",
        "IMG_PIXELS = IMG_SIZE * IMG_SIZE\n",
        "\n",
        "\n",
        "def load_raw_image(path: str) -> np.ndarray:\n",
        "    data = np.fromfile(path, dtype=np.float32)\n",
        "\n",
        "    if data.size != IMG_PIXELS:\n",
        "        raise ValueError(\n",
        "            f\"{path}: expected {IMG_PIXELS} values, found {data.size}\"\n",
        "        )\n",
        "\n",
        "    return data.reshape(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def show_reconstruction(original_path: str, reconstructed_path: str):\n",
        "    img_orig = load_raw_image(original_path)\n",
        "    img_recon = load_raw_image(reconstructed_path)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Original input\")\n",
        "    plt.imshow(img_orig, cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"VAE reconstruction\")\n",
        "    plt.imshow(img_recon, cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_sample(sample_path: str, title: str = \"VAE sample\"):\n",
        "    img = load_raw_image(sample_path)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.title(title)\n",
        "    plt.imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-LEMnvOQULE"
      },
      "outputs": [],
      "source": [
        "print(\"üìÇ Loading raw images...\")\n",
        "\n",
        "try:\n",
        "    # --- paths ---\n",
        "    base_dir = Path(\"images\")\n",
        "    original = base_dir / \"original.raw\"\n",
        "    reconstructed = base_dir / \"reconstructed.raw\"\n",
        "\n",
        "    sample_0 = base_dir / \"sample_0.raw\"\n",
        "\n",
        "    # --- visualizations ---\n",
        "    show_reconstruction(original, reconstructed)\n",
        "    show_sample(sample_0, title=\"VAE sample\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(\"‚ùå File not found:\", e)\n",
        "    print(\"Make sure you have run the 'run_sanity_check.sh' file first.\")\n",
        "except ValueError as e:\n",
        "    print(\"‚ùå Data error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8dX9If41N9n"
      },
      "source": [
        "## Micro-Benchmark Suite Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efZGaxfJ23d2"
      },
      "source": [
        "After validating the end-to-end execution of the VAE pipeline, we run a dedicated\n",
        "micro-benchmark suite to evaluate the performance of individual CUDA kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icilUO8i1UsL"
      },
      "source": [
        "The micro-benchmark script supports a configurable output directory.\n",
        "\n",
        "- **Default output directory:** `results/`\n",
        "- **Custom output directory:** specified via the `--outdir <path>` option\n",
        "\n",
        "All benchmark results are stored as CSV files inside the selected directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bP0CrUzCg4M",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!chmod +x scripts/run_micro_bench.sh\n",
        "!bash scripts/run_micro_bench.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLoQOaXrFUiR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!chmod +x scripts/run_ncu_profiling.sh\n",
        "!bash scripts/run_ncu_profiling.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potXloF3fiFJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 13,            # default per tutto\n",
        "    \"axes.labelsize\": 14,       # etichette assi (Time (ms), ecc.)\n",
        "    \"axes.titlesize\": 14,       # titolo (se lo usi)\n",
        "    \"xtick.labelsize\": 13,      # numeri e label asse X\n",
        "    \"ytick.labelsize\": 13,      # numeri asse Y\n",
        "    \"legend.fontsize\": 12,      # voci legenda\n",
        "    \"legend.title_fontsize\": 13 # titolo legenda\n",
        "})\n",
        "\n",
        "def plot_all_operations_by_strategy(\n",
        "    csv_path: str,\n",
        "    output_dir: str,\n",
        "    time_col: str = \"median_ms\",\n",
        "    err_col: str = \"mad_ms\",\n",
        "    strategy_col: str = \"strategy\",\n",
        "    op_col: str = \"operation\",\n",
        "    figsize=(15, 5),\n",
        "):\n",
        "    df = pd.read_csv(csv_path, comment=\"#\")\n",
        "\n",
        "    required = {op_col, strategy_col, time_col, err_col}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns: {missing}. Found: {list(df.columns)}\")\n",
        "\n",
        "    df = df.dropna(subset=[op_col, strategy_col, time_col, err_col]).copy()\n",
        "    if df.empty:\n",
        "        raise ValueError(\"CSV has no valid rows to plot.\")\n",
        "\n",
        "    # Strategy order = appearance order in the CSV\n",
        "    strategy_order = df[strategy_col].astype(str).drop_duplicates().tolist()\n",
        "    operations = df[op_col].astype(str).drop_duplicates().tolist()\n",
        "\n",
        "    # Handle possible duplicates defensively\n",
        "    key = [op_col, strategy_col]\n",
        "    has_dups = df.duplicated(subset=key, keep=False).any()\n",
        "\n",
        "    if has_dups:\n",
        "        data = (\n",
        "            df.groupby(key, as_index=False)\n",
        "              .agg(\n",
        "                  time=(time_col, \"mean\"),\n",
        "                  err=(err_col, \"mean\"),\n",
        "                  n=(time_col, \"size\")\n",
        "              )\n",
        "        )\n",
        "    else:\n",
        "        data = (\n",
        "            df.rename(columns={time_col: \"time\", err_col: \"err\"})\n",
        "              [[op_col, strategy_col, \"time\", \"err\"]]\n",
        "              .copy()\n",
        "        )\n",
        "        data[\"n\"] = 1\n",
        "\n",
        "    # ---- Plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    x = np.arange(len(strategy_order))\n",
        "\n",
        "    group_width = 0.85\n",
        "    bar_width = group_width / max(1, len(operations))\n",
        "\n",
        "    y_max = (data[\"time\"] + data[\"err\"]).max()\n",
        "    text_offset = max(0.01 * y_max, 0.3)\n",
        "\n",
        "    for j, op in enumerate(operations):\n",
        "        sub = data[data[op_col].astype(str) == op].set_index(strategy_col)\n",
        "\n",
        "        present_strats = [s for s in strategy_order if s in sub.index.astype(str)]\n",
        "        if not present_strats:\n",
        "            continue\n",
        "\n",
        "        idx = [strategy_order.index(s) for s in present_strats]\n",
        "        times = sub.loc[present_strats, \"time\"].to_numpy(dtype=float)\n",
        "        errs = sub.loc[present_strats, \"err\"].to_numpy(dtype=float)\n",
        "\n",
        "        offset = (j - (len(operations) - 1) / 2) * bar_width\n",
        "        xpos = np.array(idx, dtype=float) + offset\n",
        "\n",
        "        ax.bar(\n",
        "            xpos,\n",
        "            times,\n",
        "            width=bar_width * 0.95,\n",
        "            yerr=errs,\n",
        "            capsize=5,\n",
        "            label=str(op),\n",
        "        )\n",
        "\n",
        "        for xx, t, e in zip(xpos, times, errs):\n",
        "            ax.text(xx, t + e + text_offset, f\"{t:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10.5)\n",
        "\n",
        "    # Pretty x labels\n",
        "    pretty_labels = [s.replace(\"_\", \" \") for s in strategy_order]\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(pretty_labels, rotation=25, ha=\"right\")\n",
        "\n",
        "    ax.set_ylabel(\"Time (ms)\")\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "    ax.legend(title=\"Operation\")\n",
        "\n",
        "    # Extra headroom\n",
        "    top_needed = (data[\"time\"] + data[\"err\"]).max() + text_offset\n",
        "    ax.set_ylim(0, top_needed * 1.12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ---- Save\n",
        "    #out_dir = Path(output_dir)\n",
        "    #out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    #\n",
        "    #csv_name = Path(csv_path).stem\n",
        "    #out_path = out_dir / f\"{csv_name}.pdf\"\n",
        "    #\n",
        "    #plt.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
        "    #plt.close(fig)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7156K6xfj9D"
      },
      "outputs": [],
      "source": [
        "plot_all_operations_by_strategy(\"/content/VAE/results/micro_bench/csv/bench_linalg.csv\",\n",
        "                                output_dir=\"/content/VAE/results/micro_bench/plots\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def best_kernels(\n",
        "    csv_dir: str,\n",
        "    time_col: str = \"median_ms\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Per ogni CSV nella cartella:\n",
        "      - per ogni 'operation' prende la riga con tempo minimo (time_col)\n",
        "    Restituisce un unico DataFrame con:\n",
        "      source_file, operation, strategy, median_ms\n",
        "    \"\"\"\n",
        "    p = Path(csv_dir)\n",
        "    files = sorted(p.glob(\"*.csv\")) if p.is_dir() else [p]\n",
        "\n",
        "    files = [f for f in files if f.exists() and f.suffix.lower() == \".csv\"]\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in: {csv_dir}\")\n",
        "\n",
        "    out = []\n",
        "    for f in files:\n",
        "        df = pd.read_csv(f, comment=\"#\")\n",
        "\n",
        "        required = {\"operation\", \"strategy\", time_col}\n",
        "        missing = required - set(df.columns)\n",
        "        if missing:\n",
        "            raise ValueError(\n",
        "                f\"Missing columns in {f.name}: {missing}. Found: {list(df.columns)}\"\n",
        "            )\n",
        "\n",
        "        df = df.dropna(subset=[\"operation\", \"strategy\", time_col]).copy()\n",
        "        df[time_col] = df[time_col].astype(float)\n",
        "        df[\"source_file\"] = f.name\n",
        "\n",
        "        # best (min time) per operation, SOLO dentro questo file\n",
        "        best_in_file = (\n",
        "            df.sort_values([\"operation\", time_col], ascending=[True, True])\n",
        "              .drop_duplicates(subset=[\"source_file\", \"operation\"], keep=\"first\")\n",
        "              [[\"source_file\", \"operation\", \"strategy\", time_col]]\n",
        "              .rename(columns={time_col: \"median_ms\"})\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        out.append(best_in_file)\n",
        "\n",
        "    result = pd.concat(out, ignore_index=True)\n",
        "    display(result)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "pwggBu5ZSLu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_df = best_kernels(\"/content/VAE/results/micro_bench/csv\")"
      ],
      "metadata": {
        "id": "jwdhHL_SVbw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNl7g_JJ1ZNO"
      },
      "source": [
        "## Macro-Benchmatk Suite Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Implementation"
      ],
      "metadata": {
        "id": "AUVdmwkmclyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "nyA58iLccxno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"~/dataset\"\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root=PATH, train=True, download=True, transform=mnist_transform)\n",
        "test_dataset = MNIST(root=PATH, train=False, download=True, transform=mnist_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "bHDu2F0mc80X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "    super(VAE, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    # encoder\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "    self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    # decoder\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(latent_dim, hidden_dim),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(hidden_dim, input_dim),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def encode(self, x):\n",
        "    x = self.encoder(x)\n",
        "    mean, logvar = self.fc_mean(x), self.fc_logvar(x)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, var):\n",
        "    epsilon = torch.randn_like(var).to(DEVICE)\n",
        "    z = mean + var * epsilon\n",
        "    return z\n",
        "\n",
        "  def decode(self, x):\n",
        "    return self.decoder(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean, logvar = self.encode(x)\n",
        "    z = self.reparameterize(mean, torch.exp(0.5 * logvar))\n",
        "    x_hat = self.decode(z)\n",
        "    return x_hat, mean, logvar\n",
        "\n",
        "  def generate_digit(self):\n",
        "    noise = torch.randn(BATCH_SIZE, self.latent_dim).to(DEVICE)\n",
        "    return self.decoder(noise)"
      ],
      "metadata": {
        "id": "xZQzS6R8c_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import statistics\n",
        "\n",
        "def loss_function(x, x_hat, mean, logvar):\n",
        "  reconstruction_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "  KLD = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "  return reconstruction_loss + KLD\n",
        "\n",
        "def train(model, optimizer, epochs, x_dim):\n",
        "  model.train()\n",
        "  epoch_times = []\n",
        "  total_start_time = time.perf_counter()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      epoch_start_time = time.perf_counter()\n",
        "      overall_loss = 0\n",
        "\n",
        "      for batch_idx, (x, _) in enumerate(train_loader):\n",
        "          x = x.view(-1, x_dim).to(DEVICE)\n",
        "          optimizer.zero_grad()\n",
        "          x_hat, mean, logvar = model(x)\n",
        "          loss = loss_function(x, x_hat, mean, logvar)\n",
        "          overall_loss += loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      epoch_end_time = time.perf_counter()\n",
        "      epoch_duration = epoch_end_time - epoch_start_time\n",
        "      epoch_times.append(epoch_duration)\n",
        "\n",
        "      avg_loss = overall_loss / (len(train_loader.dataset))\n",
        "      print(f\"Epoch {epoch + 1} | Average Loss: {avg_loss:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "  total_end_time = time.perf_counter()\n",
        "  total_duration = total_end_time - total_start_time\n",
        "  median_time = statistics.median(epoch_times)\n",
        "\n",
        "  print(\"-\" * 30)\n",
        "  print(f\"Median time per epoch: {median_time:.4f}s\")\n",
        "  print(f\"Total training time: {total_duration:.2f}s\")\n",
        "\n",
        "  return overall_loss"
      ],
      "metadata": {
        "id": "rQuW4dX_dCTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE(input_dim=784, hidden_dim=400, latent_dim=200).to(DEVICE)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer, 100, 784)"
      ],
      "metadata": {
        "id": "IvBy6AWRdFP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Implementation"
      ],
      "metadata": {
        "id": "QpOxTw6ycqt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import ops\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "9NtCv8bCcx-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "mnist_digits = mnist_digits.reshape(-1, 784)"
      ],
      "metadata": {
        "id": "XbsTLXhGeww4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      z_mean, z_log_var = inputs\n",
        "      batch = ops.shape(z_mean)[0]\n",
        "      dim = ops.shape(z_mean)[1]\n",
        "      epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
        "      return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "iv7r5QR4eySq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(input_dim,))\n",
        "x = layers.Dense(hidden_dim)(encoder_inputs)\n",
        "x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
        "x = layers.Dense(hidden_dim)(x)\n",
        "x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "# Decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(hidden_dim)(latent_inputs)\n",
        "x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
        "x = layers.Dense(hidden_dim)(x)\n",
        "x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
        "decoder_outputs = layers.Dense(input_dim, activation=\"sigmoid\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "metadata": {
        "id": "xCWj6VbqfBwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "          z_mean, z_log_var, z = self.encoder(data)\n",
        "          reconstruction = self.decoder(z)\n",
        "\n",
        "          bce = keras.losses.binary_crossentropy(data, reconstruction)\n",
        "          reconstruction_loss = ops.mean(ops.sum(bce))\n",
        "\n",
        "          kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
        "          kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
        "\n",
        "          total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "      grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "      self.total_loss_tracker.update_state(total_loss)\n",
        "      self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "      self.kl_loss_tracker.update_state(kl_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss_tracker.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "          \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "      }"
      ],
      "metadata": {
        "id": "3IuQbtAHfGUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import statistics\n",
        "\n",
        "class TimeHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epoch_times = []\n",
        "        self.total_start_time = time.perf_counter()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.perf_counter()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        duration = time.perf_counter() - self.epoch_start_time\n",
        "        self.epoch_times.append(duration)\n",
        "        print(f\" - epoch_time: {duration:.2f}s\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        total_duration = time.perf_counter() - self.total_start_time\n",
        "        median_time = statistics.median(self.epoch_times)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(f\"Median time per epoch: {median_time:.4f}s\")\n",
        "        print(f\"Total training time: {total_duration:.2f}s\")\n",
        "        print(\"=\"*30)"
      ],
      "metadata": {
        "id": "MoQ4uz0jf3a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "time_callback = TimeHistory()\n",
        "vae.fit(mnist_digits, epochs=100, batch_size=128, callbacks=[time_callback])"
      ],
      "metadata": {
        "id": "ZZXmaalTfO7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cuda Implementation"
      ],
      "metadata": {
        "id": "UlrQPP4FcueI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36JONPXZFbhW"
      },
      "outputs": [],
      "source": [
        "!chmod +x scripts/run_macro_bench.sh\n",
        "!bash scripts/run_macro_bench.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fw6CvtaFdre"
      },
      "outputs": [],
      "source": [
        "!chmod +x scripts/run_nsys_profiling.sh\n",
        "!bash scripts/run_nsys_profiling.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kernel Fusion + Tensore Cores"
      ],
      "metadata": {
        "id": "DQWsycs82Hro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x scripts/run_macro_bench_fused_tc.sh\n",
        "!bash scripts/run_macro_bench_fused_tc.sh"
      ],
      "metadata": {
        "id": "De2HyBkFoMJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}